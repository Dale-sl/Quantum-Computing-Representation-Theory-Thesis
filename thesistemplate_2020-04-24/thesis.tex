% This is the Reed College LaTeX thesis template. Most of the work 
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
% 
% See http://web.reed.edu/cis/help/latex.html for help. There are a 
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
% 
% Any line that starts with a percent symbol is a comment. 
% They won't show up in the document, and are useful for notes 
% to yourself and explaining commands. 
% Commenting also removes a line from the document; 
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in 
% the 2002-2003 Senior Handbook. Ask a librarian to check the 
% document before binding. -SN

%% 
%% Preamble
%% 
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%% 
\usepackage{graphicx,latexsym} 
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace}
\usepackage{braket}
\usepackage{mathtools}
\usepackage[hyphens]{url}
\usepackage{rotating}
\usepackage{ytableau}
\usepackage[capitalise]{cleveref}

% make notes fit in margins
\usepackage{todonotes}
\setlength{\marginparwidth}{2cm}

% make paragraphs nicer
% \usepackage[skip=40pt plus1pt, indent=40pt]{parskip}

% \usepackage{natbib}
\usepackage[numbers]{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new 
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the 
% bibliography is included. 
% \usepackage{biblatex-chicago}
% \bibliography{thesis}

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

% beware, math preamble paste begins

\theoremstyle{plain}   % This is the default, anyway
\newtheorem{thm}{Theorem}[section] % numbered theorem
\makeatletter\let\c@thm\c@thm\makeatother
\newtheorem{cor}{Corollary}[section]
\makeatletter\let\c@cor\c@thm\makeatother
\newtheorem{lemma}{Lemma}[section]
\makeatletter\let\c@lemma\c@thm\makeatother
\newtheorem{prop}{Proposition}[section]
\makeatletter\let\c@prop\c@thm\makeatother
\newtheorem{claim}{Claim}[section]
\makeatletter\let\c@claim\c@thm\makeatother


\newtheorem*{unnumberedtheorem}{Theorem}  % unnumbered theorem
\newtheorem*{unnumberedcorollary}{Corollary}
\newtheorem*{unnumberedlemma}{Lemma}
\newtheorem*{unnumberedproposition}{Proposition}
\newtheorem*{unnumberedclaim}{Claim}

\theoremstyle{definition}

\newtheorem{defn}{Definition}[section]
\makeatletter\let\c@defn\c@thm\makeatother
\newtheorem{const}{Construction}[section]
\makeatletter\let\c@const\c@thm\makeatother
\newtheorem{notn}{Notation}[section]
\makeatletter\let\c@notn\c@thm\makeatother
\newtheorem{outline}{Proof Outline}[section]
\makeatletter\let\c@outline\c@thm\makeatother
\newtheorem{post}{Postulate}[section]
\makeatletter\let\c@outline\c@thm\makeatother


\theoremstyle{remark}

\newtheorem{rem}{Remark}[section]
\makeatletter\let\c@rem\c@thm\makeatother
\newtheorem{ex}{Example}[section]
\makeatletter\let\c@ex\c@thm\makeatother
\newtheorem{observation}{Observation}[section]
\makeatletter\let\c@observationn\c@thm\makeatother

\newtheorem*{unnumberedtheoremremark}{Remark}
\newtheorem*{unnumberedtheoremexample}{Example}
\newtheorem*{unnumberedtheoremdefinition}{Defintion}

\makeatletter
\let\c@equation\c@thm
\numberwithin{equation}{section}
\makeatother

\def\cA{\mathcal{A}}\def\cB{\mathcal{B}}\def\cC{\mathcal{C}}\def\cD{\mathcal{D}}\def\cE{\mathcal{E}}\def\cF{\mathcal{F}}\def\cG{\mathcal{G}}\def\cH{\mathcal{H}}\def\cI{\mathcal{I}}\def\cJ{\mathcal{J}}\def\cK{\mathcal{K}}\def\cL{\mathcal{L}}\def\cM{\mathcal{M}}\def\cN{\mathcal{N}}\def\cO{\mathcal{O}}\def\cP{\mathcal{P}}\def\cQ{\mathcal{Q}}\def\cR{\mathcal{R}}\def\cS{\mathcal{S}}\def\cT{\mathcal{T}}\def\cU{\mathcal{U}}\def\cV{\mathcal{V}}\def\cW{\mathcal{W}}\def\cX{\mathcal{X}}\def\cY{\mathcal{Y}}\def\cZ{\mathcal{Z}}

\def\AA{\mathbb{A}} \def\BB{\mathbb{B}} \def\CC{\mathbb{C}} \def\DD{\mathbb{D}} \def\EE{\mathbb{E}} \def\FF{\mathbb{F}} \def\GG{\mathbb{G}} \def\HH{\mathbb{H}} \def\II{\mathbb{I}} \def\JJ{\mathbb{J}} \def\KK{\mathbb{K}} \def\LL{\mathbb{L}} \def\MM{\mathbb{M}} \def\NN{\mathbb{N}} \def\OO{\mathbb{O}} \def\PP{\mathbb{P}} \def\QQ{\mathbb{Q}} \def\RR{\mathbb{R}} \def\SS{\mathbb{S}} \def\TT{\mathbb{T}} \def\UU{\mathbb{U}} \def\VV{\mathbb{V}} \def\WW{\mathbb{W}} \def\XX{\mathbb{X}} \def\YY{\mathbb{Y}} \def\ZZ{\mathbb{Z}}  

\def\fa{\mathfrak{a}} \def\fb{\mathfrak{b}} \def\fc{\mathfrak{c}} \def\fd{\mathfrak{d}} \def\fe{\mathfrak{e}} \def\ff{\mathfrak{f}} \def\fg{\mathfrak{g}} \def\fh{\mathfrak{h}} \def\fj{\mathfrak{j}} \def\fk{\mathfrak{k}} \def\fl{\mathfrak{l}} \def\fm{\mathfrak{m}} \def\fn{\mathfrak{n}} \def\fo{\mathfrak{o}} \def\fp{\mathfrak{p}} \def\fq{\mathfrak{q}} \def\fr{\mathfrak{r}} \def\fs{\mathfrak{s}} \def\ft{\mathfrak{t}} \def\fu{\mathfrak{u}} \def\fv{\mathfrak{v}} \def\fw{\mathfrak{w}} \def\fx{\mathfrak{x}} \def\fy{\mathfrak{y}} \def\fz{\mathfrak{z}}
\def\fN{\mathfrak{N}}
\def\fgl{\mathfrak{gl}}  \def\fsl{\mathfrak{sl}}  \def\fso{\mathfrak{so}}  \def\fsp{\mathfrak{sp}}  
\def\GL{\mathrm{GL}} \def\SL{\mathrm{SL}}  \def\SP{\mathrm{SL}}\def\OG{\mathrm{O}}

\def\aa{\mathbf{a}} \def\bb{\mathbf{b}} \def\cc{\mathbf{c}} \def\dd{\mathbf{d}} \def\ee{\mathbf{e}} \def\ff{\mathbf{f}}
% \def\gg{\mathbf{g}}
\def\hh{\mathbf{h}} \def\ii{\mathbf{i}} \def\jj{\mathbf{j}} \def\kk{\mathbf{k}}
% \def\ll{\mathbf{l}}
\def\mm{\mathbf{m}} \def\nn{\mathbf{n}} \def\oo{\mathbf{o}} \def\pp{\mathbf{p}} \def\qq{\mathbf{q}} \def\rr{\mathbf{r}} \def\ss{\mathbf{s}}  \def\uu{\mathbf{u}} \def\vv{\mathbf{v}} \def\ww{\mathbf{w}} \def\xx{\mathbf{x}} \def\yy{\mathbf{y}} \def\zz{\mathbf{z}}
\def\zzero{\mathbf{0}}

\def\<{\langle} \def\>{\rangle}
\def\Aut{\mathrm{Aut}}
\def\ch{ \stackrel}
\def\col{\mathrm{col}}
\def\dim{\mathrm{dim}} 
\def\End{\mathrm{End}} 
\def\ev{\mathrm{ev}} 
\def\f{\varphi}
\def\gcd{\mathrm{gcd}}
\def\half{\hbox{$\frac12$}}
\def\Hom{\mathrm{Hom}}
\def\img{\mathrm{img}}
\def\id{\mathrm{id}} 
\def\Inn{\mathrm{Inn}}
\def\lcm{\mathrm{lcm}}
\def\normeq{\trianglelefteq}
% \def\ch{ \stackrel{\mathrm{ch}}{\trianglelefteq}}
% \def\normeq{\trianglelefteq}
\def\nul{\mathrm{nullity}}
\def\row{\mathrm{row}}
\def\rk{\mathrm{rank}}
\def\sgn{\mathrm{sgn}}
\def\sp{\mathrm{span}}
\def\supp{\mathrm{supp}}
\def\Syl{\mathrm{Syl}}
\def\tr{\mathrm{tr}} 
\def\vep{\varepsilon}

% \usepackage{mathabx}
\def\acts{\circlearrowright} %group action

% Cleveref definitions
% \crefname{lemma}{Lemma}{Lemmas}
% \crefname{thm}{Theorem}{Theorems}
% \crefname{defn}{Definition}{Definitions}
% \crefname{notn}{Notation}{Notations}
% \crefname{const}{Construction}{Constructions}
% \crefname{prop}{Proposition}{Propositions}
% \crefname{rem}{Remark}{Remarks}
% \crefname{cor}{Corollary}{Corollaries}
% \crefname{equation}{Diagram}{Diagrams}
% \crefname{ex}{Example}{Examples}

% Arrays:
\newcommand\Pmatrix[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand\smatrix[1]{\text{\small$\begin{pmatrix}#1\end{pmatrix}$}}
\newcommand\fmatrix[1]{\text{\footnotesize$\begin{pmatrix}#1\end{pmatrix}$}}
\newcommand\tmatrix[1]{\text{\tiny$\begin{pmatrix}#1\end{pmatrix}$}}


% math preamble ends






\title{Representation Theory in Quantum Computing}
\author{Dale D. Schandelmeier-Lynch}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2025}
\division{Mathematical and Natural Sciences}
\advisor{Jamie Pommershiem}
% If you have two advisors for some reason, you can use the following
\altadvisor{Greg Anderson}
%%% Remember to use the correct department!
\department{Mathematics and Computer Science}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
\approvedforthe{Committee}

\setlength{\parskip}{0pt}
%% 
%% End Preamble
%% 
%% The fun begins:
\begin{document}

  \maketitle
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
  \chapter*{Acknowledgements}
  I want to thank a few people.

  % The preface is optional
  % To remove it, comment it out or delete it.
  \chapter*{Preface}
  This is an example of a thesis setup to use the reed thesis document class.



  \chapter*{List of Abbreviations}
  You can always change the way your abbreviations are formatted. Play around with it yourself, use tables, or come to CUS if you'd like to change the way it looks. You can also completely remove this chapter if you have no need for a list of abbreviations. Here is an example of what this could look like:

  \begin{table}[h]
    \centering % You could remove this to move table to the left
    \begin{tabular}{ll}
      \textbf{ABC}  	&  American Broadcasting Company \\
      \textbf{CBS}  	&  Columbia Broadcasting System\\
      \textbf{CDC}  	&  Center for Disease Control \\
      \textbf{CIA}  	&  Central Intelligence Agency\\
      \textbf{CLBR} 	&  Center for Life Beyond Reed\\
      \textbf{CUS}  	&  Computer User Services\\
      \textbf{FBI}  	&  Federal Bureau of Investigation\\
      \textbf{NBC}  	&  National Broadcasting Corporation\\
    \end{tabular}
  \end{table}


  \tableofcontents
  % if you want a list of tables, optional
  \listoftables
  % if you want a list of figures, also optional
  \listoffigures

  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

  % The abstract is not required if you're writing a creative thesis (but aren't they all?)
  % If your abstract is longer than a page, there may be a formatting issue.
  \chapter*{Abstract}
  The preface pretty much says it all.
  \onehalfspacing
  % \doublespacing



  \chapter*{Introduction}
  \addcontentsline{toc}{chapter}{Introduction}
  \chaptermark{Introduction}
  \markboth{Introduction}{Introduction}

  Theoretical quantum computing often employs a model\todo{maybe this shouldn't be called a model}
  of asymptotic analysis known as \emph{oracle problems} as an abstraction to make comparisons between classical and quantum computers easier.
  An oracle is a theoretical function whose inner workings cannot be observed or manipulated; algorithms are only allowed to choose an input to the oracle and recieve its output.
  The goal when given an oracle problem is to create an algorithm which learns a given characteristic of the function hidden by the oracle;
  we usually assume that the function hidden by the oracle is sampled randomly from a distribution known to the algorithm.
  A simple example is as follows. For a function $f: \{a_0, a_1 \} \to \ZZ / 2\ZZ$ uniformly sampled from the space of possible functions, determine the mod $2$ sum $a_0 \oplus a_1$.
  Here we define the oracle $O$ to offer information to the algorithm through the black-box function $O(x) \coloneq f(x)$.
  A classical algorithm would have to make $2$ queries to $O$ for exact learning, as it must query every element of the codomain to learn the values of $a_o$ and $a_1$.
  As it turns out, a well known quantum algorithm called Deutsch's algorithm can learn the value of $a_0 \oplus a_1$ with certainty after just $1$ query! \par
  Also of interest in oracle problems is \emph{bounded error} learning, where we consider how many queries are needed until an algorithm has some $\geq \frac{1}{2}$ chance of guessing the characteristic correctly.
  Contrast the previous example with a problem where the algorithm has to learn $f$ exactly; before making any queries the classical algorithm has a $\frac{1}{2}$ of guessing $a_0 \oplus a_1$ correctly,
  and this doesn't change after making a query. On the other hand learning the exact function has a $\frac{1}{4}$ chance of success with $0$ queries, and a $\frac{1}{2}$ chance of success with $1$ query.
  If we considered an extreme version of this problem, $f: \{a_0, a_1 \} \to \ZZ / 1000\ZZ$, then we observe that a classical algorithm has a near-zero chance of guessing the function before it makes its final query.
  For most oracle problems, this pattern holds true classically; the algorithms knows nearly nothing about the function until it knows the whole function.
  Effectively, bounded error learning is equivalent to exact learning in this case.
  Shockingly, quantum algorithms have a different tendency.
  The function will be nearly unknown for the first few queries, but after some number of queries asymptotically smaller than the number of queries needed for exact learning nearly the whole function will be known!
  We are thus concerned with comparing the exact classical query complexity with both the exact and bounded error quantum query complexity of a given oracle problem.
  Of note is that we are proving we have found a neccesary and sufficent number of queries to solve the problem; we have an algorithm which solves the problem with the given asymptotics, and it is optimal,
  meaning no algorithm can do asymptotically better.
  % In fact, we will be able to prove that there exists an algorithm which has a given query complexity, and that furthermore no algorithm exists which can do better --- the algorithm will be optimal.
  \par
  Our oracle problem comes from \cite{copeland}, as a specfication of the more general coset identification problem:
  suppose a group $G$ acts by permutations on a finite set $\Omega$ (we call $\Omega$ a $G$-set).
  An algorithm is given access to an oracle which takes an element $\omega \in \Omega$ and returns $a \cdot \omega$ for some hidden group element (uniformly sampled) $a \in G$. The goal is to guess
  the hidden element $a \in G$.
  We will be concerned with the groups $S_n$ and $A_n \leq S_n$ acting on the set of $k$ element subsets of $n$, and on the set of regular partitions of $n$ into $b$ parts of size $a$.
  \par
  To answer these problems we will use a crucial result from \cite{copeland} which enables us to determine optimal quantum query complexity with a correspondence.
  Taking the tensor product of a representation derived from our $G$-set $\Omega$ with itself repeatedly will be equivalent to making oracle queries, turning our quantum computing problem
  into one of representation theory.
  By understanding the representation corresponding to $\Omega$ and the way it tensors with itself, we will be able to find the query complexity.
  Because our $G$-sets have $S_n$ and $A_n$ as groups, we will be deeply concerned with the representation theory of $S_n$.
  
  
  \chapter{Background}
  We first introduce the quantum computing and representation theory neccesary to understand the problem statement in context.
  \section{Quantum Computing}
  Quantum computing utilizes the nonclassical nature of quantum mechanics to achieve asymptotic improvements over classical algorithms.
  Not all problems admit a quantum ``speedup''; we will be more specific about our specific computational framework after the neccesary background.
  To illuminate the physical-mathematical framework in which quantum computing takes place, we will expound the postulates of quantum mechanics and the associated linear algebra from \cite{nielsen2010}[Chapter 2].
  \subsection{Quantum Mechanics}
  Due to theorem 4.2 from \cite{copeland}, it turns out that our results rely almost entirely on representation theory;
  nevertheless we introduce the 4 fundemental postulates from chapter 2 of \cite{nielsen2010} to provide context for the quantum model of computation.
  Throughout this section we use the ``bra-ket'' (or Dirac) notation to represent vectors in vector spaces. This is inherited from physics and uses the symbols
  $\ket{\cdot}$ called ket, $\bra{\cdot}$ called bra, and $\braket{\cdot | \cdot}$ called braket (as we have ``bracketed'' the symbols); 
  \[ \ket{\psi} \]
  is a vector we have labeled $\psi$. In quantum computing we are concerned with complex vector spaces by postulate; we need to define inner products both for this postulate and to elaborate on bra-ket notation.
  \begin{defn}[{Inner product, \cite[2.1.4]{nielsen2010}}]
    A function $(\cdot, \cdot): V \times V \to \CC$ is a (complex) inner product if it satisfies the following requirements:
    \begin{enumerate}
    \item $(\cdot, \cdot)$ is linear in the second argument:
      \[ \left( \ket{v}, \sum_i \lambda_i \ket{w_i} \right) = \sum_i \lambda_i \left( \ket{v}, \ket{w_i} \right).\]
    \item $(\ket{v}, \ket{w}) = (\ket{v}, \ket{w})^*$.
    \item $(\ket{v}, \ket{v}) \geq 0$ with equality if and and only if $\ket{v} = 0$.
    \end{enumerate}
  \end{defn}
  The intuition of braket notation is that $\bra{\phi}$ is the dual of the vector $\ket{\phi}$, which in the corresponding matrix presentation
  of vectors corresponds to turning a column vector into a row vector. Therefore taking the standard matrix product
  \[\bra{\phi} \ket{\psi}\]
  is equivalent to the standard inner product $(\ket{\phi}, \ket{\psi}) = \sum_{k=1}^n \phi_k^* \psi_k$, which we notate as
  \[\braket{\phi | \psi};\]
  here the symbol $\cdot^*$ represents the unary complex conjugation operation.
  The elegance of braket notation, apart from saving space, is that we can easily tell the output of complicated linear expressions;
  it easy to see that $\braket{\phi | \psi} \braket{x | y}$ equals a scalar, while it takes a little thinking to see that
  \[
    \begin{bmatrix}
      x & y
    \end{bmatrix}
    \begin{bmatrix}
      x \\ y
    \end{bmatrix}
    \begin{bmatrix}
      x & y
    \end{bmatrix}
    \begin{bmatrix}
      x \\ y
    \end{bmatrix}
  \]
  equals a scalar.
  \par
  We are now ready to introduce our first postulate. As mathematicians we needn't worry about how pysicists experimentally constructed these postulates, although their
  (approximate) truth will be neccesary if we are to physically realize quantum computers and employ the alogrithms we design.
  If we take these postulates at face value, phenomenon that are strange from a classical physics standpoint feel like natural consequences of the mathematics;
  in that sense it can be helpful to abstract away your physical intution and instead focus on the consequences of these postulates when we start doing computation.
  \begin{post}[ {\cite[2.2.1]{nielsen2010}} ]
    Associated to any isolated physical system is a complex vector space
    with inner product (that is, a Hilbert space) known as the \emph{state space} of the
    system. The system is completely described by its \emph{state vector}, which is a unit
    vector in the system's state space.
  \end{post}
  Here unit vector refers to a vector with norm $1$; the standard norm we employ is $|| \ket{\psi} || \coloneq \sqrt{\braket{\psi| \psi}}$.
  Note that quantum mechanics doesn't tell us the state space or state vector of a particular system; it only describes in general the characteristics systems share.
  In our quantum computing model we regard these physical systems as memory and perform operations on this memory through the appropriate mathematical morphism, which in this case are linear operators.
  A minor detail is that Hilbert spaces have more properties when they are infinite dimensional, but in our case we will only ever use finite memory and so our Hilbert spaces are always finite dimensional. \par
  The simplest quantum mechanical system is the \emph{qubit}. A qubit resides in $\CC^2$ and so has a two dimensional state space.
  We use $\ket{0}$ and $\ket{1}$ to label the elements of an orthonormal basis for the qubit; in practice we let
  \[\ket{0} =
    \begin{pmatrix}
      1 \\
      0
    \end{pmatrix},
    \quad
    \ket{1} =
    \begin{pmatrix}
      0 \\
      1
    \end{pmatrix}.
  \]
  Note that $\ket{0}$ is not the zero vector $0$; we use $\ket{0}$ despite this overlap in reference to the two states of a bit, $0$ and $1$.
  What makes the qubit interesting in contrast is that we are not limited to state vectors $\ket{0}$ and $\ket{1}$; we know from linear algebrea that
  we can express any element of the state basis as
  \[\ket{\psi} = a \ket{0} + b \ket{1}, \]
  where $a$ and $b$ are in the complex numbers. Even with the condition that $\ket{\psi}$ be a unit vector
  the state of the qubit is described by $4$ real numbers (that is $x +iy$ where $x,y \in \RR$ for both $a$ and $b$) instead of one bit! In practice we can only physically measure $3$ of these values,
  but nevertheless a qubit can take on far more values than a bit because it can exist in a superposition of the states $\ket{0}$ and $\ket{1}$; that is both $a$ and $b$ can be nonzero at the same time.
  In our case we will be using the state space $\CC \Omega$ with standard basis $\{ \ket{\omega} | \omega \in \Omega\}$; you can imagine we have defined an abelian group structure on $\Omega$ and so we are
  considering the group algebra $\CC \Omega$ by linearization. \par
  We are now ready for our next postulate:
  \begin{post}[{\cite[2.2.2]{nielsen2010}}]
    The evolution of a \emph{closed} quantum system is described by a \emph{unitary transformation}. That is, the state $\ket{\psi}$ of the sysem at time $t_1$ is related to the state
    $\ket{ \psi^{'} }$ of the system at time $t_2$ by a unitary operator $U$ which depends only on the times $t_1$ and $t_2$;
    \[ \ket{\psi^{'}} = U \ket{\psi}.\]
  \end{post}
  With operators we can define the last piece of the bra-ket puzzle. If we consider the vector $\ket{v}$ from a Hilbert space $V$, and the vector $\ket{w}$ from a Hilbert space $W$,
  then we define the out product notation for a linear operator $\ket{w} \bra{v} : V \to W$ as
  \[ (\ket{w} \bra{v}) (\ket{v^{'}}) \coloneq \ket{w} \braket{v|v^{'}} = \braket{v|v^{'}} \ket{w}.\]
  We can thus interpret the notation $\ket{w} \braket{v|v^{'}}$ in two ways: as a linear operator fed $\ket{v^{'}}$, and as the vector $\ket{w}$ scaled by the inner product of $\braket{v|v^{'}}$.
  In terms of matrix multplication we see here the associativity property at work; the product of the column and row vectors $\ket{w} \bra{v}$ creates a matrix which then acts as a linear operator on
  $\ket{v^{'}}$, or the product of row and column vectors creates a scalar which then scales $\ket{w}$.
  \par
  To define a unitary operator, we first need the \emph{adjoint} or \emph{Hermitian conjugate} \cite[2.1.6]{nielsen2010} of an operator.
  This the unique operator $A^\dag$ such that for all vectors $\ket{v_1}, \ket{v_2} \in V$,
  \[(\ket{v_1}, A \ket{v_2}) = ( A^\dag \ket{v_1}, \ket{v_2} ).\]
  Here the operation is the inner product; by convention we define $(\ket{v})^\dag \coloneq \bra{v}$ so that $(A \ket{v_1})^\dag \ket{v_2} = \braket{v_1 | A^\dag |v_2 }$.
  When we have a matrix representing an operator $A$, the standard definition of the adjoint is to take the conjugate and then the tranpose of $A$: $A^\dag \coloneq (A^*)^T$.
  When $A^\dag = A$, we call $A$ \emph{Hermitian} or \emph{self-adjoint}.
  As a broader class we say that an operator $A$ is \emph{normal} if $AA^\dag = A^\dag A$; it is immediate that Hermitian operators are normal.
  Finally we say an operator $U$ is unitary if $U^\dag U = I$, where $I$ is the identity operator (or matrix); for finite vector spaces it can be proven that this implies $ U U^\dag = I$, so unitary
  operators are also normal.
  We care about unitary products because they preserve inner products; observe that
  \begin{align*}
    & (U \ket{v}, U \ket{w}) = \braket{v | U^\dag U  | w} = \braket{v | I | w} = \braket{v | w}  & & \text{\cite[2.36]{nielsen2010}.}
  \end{align*}


  Postulate 2 demands that operators be unitary so that the unit condition of the state vector is maintained. \par
  We are now ready for postulate 3, which tells us what happens when an isolated quantum system is measured.
  To measure a quantum system it must be exposed to an external physcial system, and so the rule of unitary evolution from postulate 2 no longer neccearily applies.
  \begin{post}[{ \cite[2.2.3]{nielsen2010} }]
    Quantum measurements are described by a collection $\{M_m\}$ of \emph{measurement operators}.
    These are operators acting on the state space of the system being measured.
    The index $m$ refers to the measurement outcomes that may occur in the experiment.
    If the state of the quantum system is $\ket{\psi}$ immediately before the measurement then the probability that result $m$ occurs is given by
    \[ p(m) = \braket{ \psi | M_m^\dag M_m | \psi},\]
    and the state of the system after the measurement is
    \[ \dfrac{M_m \ket{\psi}}{\sqrt{\braket{\psi | M_m^\dag M_m | \psi}}}.\]
    The measurement operators satisfy the \emph{completeness equation},
    \[ \sum_m M_m^\dag M_m = I.\]
    The completeness equation expresses the fact that probabilities sum to one:
    \[ 1 = \sum_m p(m) = \sum_m \braket{\psi | M_m^\dag M_m | \psi}.\]
  \end{post}
  The takeaways from this postulate are twofold. Measurements in a quantum system can change the state of the state vector; information can be lost by performing a measurement.
  From the real numbers contained in a state vector --- 3 in the case of the qubit --- we can only observe a finite number of states.
  In fact, if we are trying to distinguish between states that are not orthonormal, then we lose the guarentee of distinguishability.
  Because some state $\ket{\psi}$ is not orthonormal to some other distinict state $\ket{\phi}$, there is a component of $\ket{\psi}$ that is orthogonal to $\ket{\phi}$ and a component
  that is parallel to $\ket{\phi}$; therefore when you measure a system with state vector $\ket{\psi}$ there is a nonzero chance of it being misidentified as $\ket{\phi}$.
  This means that we can only perfectly distinguish between a number of states up to the dimension of the state space.
  Furthermore cascaded measurements $\{L_l \}$ and $\{M_m\}$ are equivalent to a single measurement with operators $\{N_{lm}\}$ where $N_{lm} \coloneq M_m L_l$ \cite[2.57]{nielsen2010},
  so if we don't care about the evolution of the system over time there is no reason to not take all the measurements we want to at once. \par
  This leads us to the concept of the POVM, or ``positive operator-valued measure.''
  If we perform a measurement on a state vector $\ket{\psi}$ with measurement operators $M_m$, then we know the probability of outcome $m$ is given by $p(m) = \braket{\psi | M_m^\dag M_m | \psi}$.
  Now if we define
  \[ E_m \coloneq M_m^\dag M_m,\]
  we can conclude that $\{E_m\}$ is a set of positive operators such that $\sum_m E_m = I$ and $p(m) = \braket{\psi | E_m | \psi}$.
  Therefore this set is sufficent to determine the probability of each measurement outcome if we don't care about the post-measurement state \cite[2.2.6]{nielsen2010}.
  In fact we can take this the other direction and define a POVM to be any set of operators such that each operator is positive and the
  completeness relation $\sum_m E_m = I$ is obeyed.
  This is legitimate because if we define measurements $M_m \coloneq \sqrt{E_m}$, we see that $\sum_m M_m^\dag M_m = \sum_m E_m = I$,
  so we have constructed a set of measurements $\{M_m\}$ with POVM $\{E_m\}$.
  We prefer POVMs to measures as a convience, because when we do computation we take one measurement at the end and discard the state vector. \par
  Our last postulate tells us how to describe quantum systems made out of multiple distinct systems; this is useful in computation because we model adding memory as
  composing more physical systems.
  \begin{post}[{ \cite[2.2.8]{nielsen2010} }] 
    The state space of a composite physical system is the tensor product of the state spaces of the component physical systems. Moreover,
    if we have systems numbered $1$ through $n$, and system number $i$ is in the state $\ket{\psi_i }$, then the joint state of the total system is $\ket{\psi_1} \otimes \ket{\psi_2} \otimes \cdots \otimes \ket{\psi_n}$.
  \end{post}
  A tensor product is an operation on vector spaces analagous to ``multiplying'' them;
  In the same way that a cartesian product is an operation on sets that changes what the elements of the resulting set look like (turns them into tuples),
  the tensor product of the spaces will change the vectors into ``tensor products'' in a different sense.
  \par
  The following definitions are from \cite[2.1.7]{nielsen2010}.
  Suppose we have hilbert spaces $V$ and $W$ --- although tensor products can be defined on plain vector spaces --- of dimension $m$ and $n$ respectively.
  Then $V \otimes W$ is an $mn$ dimensional vector space.
  As an analogue for finite sets $X: |X| = m$ and $Y: |Y| = n$ the cartesian product has cardinality $|X \times Y| = mn$.
  The elements of this vector space are linear combinations of tensor products $\ket{v} \otimes \ket{w}$
  where $\ket{v} \in V$ and $\ket{w} \in W$.
  The tensor product is akin to a tuple in that elements from the first vector space go in the first index, elements from the second vector space go in the second index, et cetra.
  If $\{\ket{i}_n \}$ and $\{\ket{j}_n \}$ are (preferably but not neccesarily orthonormal) bases for the spaces $V$ and $W$ then $\{ \ket{i} \otimes \ket{j} \}_{n, m}\}$ is a basis for $V \otimes W$.
  We abbreviate tensor products as $\ket{v} \ket{w}$ or $\ket{vw}$ for convinence.
  For example if $V$ is a qubit then $V \otimes V$, also notated as $V^{\otimes 2}$ to indicate it has been tensored with itself (akin to squaring) contains the element
  $\ket{0} \otimes \ket{0} + \ket{1} \otimes \ket{1} = \ket{0 0} + \ket{1 1}$.
  We define the tensor product at the element (vector) level to have the following properties:
  \begin{enumerate}
  \item For an arbitrary scalar $z$ and vectors $\ket{v} \in V$ and $\ket{w} \in W$,
    \[ z ( \ket{v} \otimes \ket{w}) = (z \ket{v}) \otimes \ket{w} = \ket{v} \otimes (z \ket{w }).\]

  \item
    For an arbitrary $\ket{v_1}, \ket{v_2} \in V$ and $\ket{w} \in W$,
    \[ (\ket{v_1} + \ket{v_2}) \otimes \ket{w} = \ket{v_1}  \otimes \ket{w} + \ket{v_2} \otimes \ket{w}. \]
  \item
    For an arbitrary $\ket{v_1}, \ket{v_2} \in V$ and $\ket{w} \in W$,
    \[ (\ket{v_1} + \ket{v_2}) \otimes \ket{w} = \ket{v_1}  \otimes \ket{w} + \ket{v_2} \otimes \ket{w}. \]
  \item
    For an arbitrary $\ket{v} \in V$ and $\ket{w_1}, \ket{w_2} \in W$,
    \[ \ket{v} \otimes (\ket{w_1} + \ket{w_2})  = \ket{v}  \otimes \ket{w_1} + \ket{v} \otimes \ket{w_2}. \]
  \end{enumerate}
  These distributivity laws intuitively make tensor products function like multiplication between vectors.
  Furthermore, we can define linear operators on tensor products which inherit from linear operators on the tensored spaces;
  let $\ket{v} \in V, \ket{w} \in W$, and let $A$ and $B$ be linear operators on $V$ and $W$.
  Then we define the linear operator $A \otimes B$ in $V \otimes W$ by the equation
  \[(A \otimes B) (\ket{v} \otimes \ket{w}) \coloneq A  \ket{v} \otimes B \ket{w}.\]
  This definition of $A \otimes B$ now naturally extends linearly to all the elements of $V \otimes W$; that is
  \[(A \otimes B) (\sum_i a_i \ket{v_i} \otimes \ket{w_i}) \coloneq \sum_i a_i A\ket{v_i} \otimes B\ket{w_i}.\]
  It can be shown that $A \otimes B$ thus defined is a well-defined linear operator.
  Finally a natural inner product can be inherited from the tensored spaces; we define this product as 
  \[ \left( \sum_i a_i \ket{v_i} \otimes \ket{w_i}, \sum_j b_j \ket{v^{'}_j} \otimes \ket{w^{'}_j}  \right) \coloneq
    \sum_{i, j} a_i^* b_j \braket{v_i  | v^{'}_j} \braket{w_i  | w^{'}_j}. \]
  This function can also be shown to be a well-defined inner product. Equiped with this inner product $V \otimes W$ is now an inner product/Hilbert space, and it inherits our previously established structure of
  an adjoint, unitarity, normality, Hermanicity, et cetra.
  \subsection{Our Computational Model}
  We have now covered all the postulates of quantum mechanics! The standard model of quantum computation is simple: we start with a state space $(\CC^{2})^{\otimes n} \otimes (\CC^{2})^{\otimes a}$ which corresponds
  to an input requiring $n$ qubits to store and an extra workspace of $a$ ancilla qubits --- usually of constant size --- needed as extra space for certain computations.
  We can assume that the memory begins in any desired state $\ket{\psi}$ as such a state can be initialized by applying a constant number of unitary operators \todo{is this true? maybe only takes one?}.
  Unitary operators are then applied to do some kind of computation, and a POVM is taken to measure the output.
  Instead of worrying about the time or space complexity of this model, we will be concerned with query complexity; this simplifies complexity to the question of how many times a given black box function
  (the ``oracle'') is applied as a unitary operator.
  Because this model is non-uniform --- the operators applied change as the input size changes --- we describe a quantum algorithm as a recipe to construct the neccesary sequence of operations
  based on the input size. This model lends itself to representation as a circuit diagram, where reading from left to right we initialize qubits as seperate inputs, apply unitary operations
  to qubits, and then perform a measurement; an example is depicted below.
  \missingfigure{ example circuit diagram. }
  The model we employ in our research is similar, but instead uses the state space $\CC\Omega \otimes \CC^N$; here we are taking the group ring $\CC \Omega$ where $\Omega$ is a $G$-set with cyclic group structure
  as a single register. You can imagine it as a $|\Omega|$-dit because $\CC\Omega$ has $\ket{\omega} \in \Omega$ as a basis, but endowed with extra structure because the action of $G$ on $\Omega$ naturally corresponds
  to a group of unitary operators on $\CC\Omega$. The space $\CC^N$ is simply ancilla but regarded as one $N$-dit rather than multiple qubits. Note that the postfix ``-dit'' here indicates a register of a given dimension
  (d for dimension),
  whereas qubit refers to a register of dimension $2$ (as ``bi-'' means $2$).
  To understand why we use this model, we will need an introduction to representation theory.
  \section{Introductory Representation Theory}
  \todo{this is copy-pasted, needs to be modified to flow with rest of background.}
  Our results and model employ a field of abstract algebra known as representation theory, which is concerned with
  correspondences between groups and linear operators. By turning group elements into (unitary) linear operators, we will be able to use them as gates in our quantum computing model.
  Our introduction here will be focused on laying the groundwork for the representation theory of $S_n$, the subject in representation theory relevant to our results.
  \par
  Our first definition will be specific to the set of invertible $d \times d$ matrices with entries in $\CC$.
  This is the full complex matrix algebra, because we have a vector space of matrices scaled by the the complex numbers
  with a multiplication defined by matrix multiplication.
  \begin{defn}[{\cite[Definition 1.2.1]{sagan}}]
    \todo{ citation style in bibtex file is different here}
    A \emph{representation} (more specifically a \emph{matrix representation}) of a group $G$ is a group homomorphism
    \[\rho: G \to \GL_d\]
    where $\GL_d$ is the complex general linear group of degree $d$; that is, the set of
    invertible $d \times d$ matrices from $\CC^d$ to $\CC^d$. \\
    To break down the definition of a group homomorphism, this means that
    \[ \rho(e) = I_d\]
    where $e$ is the identity of the group, and $I_d$ is the identity matrix of degree $d$ in $\CC$, and
    \[\rho(g)\rho(h) = \rho(gh) \text{ for all $g,h \in G$.}\]
    This means that multiplying group elements and then taking their matrix representation is equivalent to
    taking their matrix representations and performing matrix multiplication on them.
  \end{defn}
  Note that representations can be defined for other fields like $\RR$ or $\ZZ/P\ZZ$, and doing so changes the core theorems of the theory;
  we won't be concerned with this because our quantum mechanical postulates demand we use $\CC$.
  A point of confusion with representations is that they are defined to be \emph{homomorphisms}, not \emph{monomorphisms}.
  We might intuitively imagine that a matrix representation is equivalent to the group just with matrices instead of group elements,
  but information/group elements can be lost by mapping nontrivial elements of the group to the identity matrix in the linear group
  (that is, a representation can have nontrivial kernel).
  Furthermore, additional representations can be created by choosing a different degree or performing a change of basis on a given representation.
  A small degree can possibly restrict the possible representations to exclude monomorphisms.
  On the other hand, a change of basis shouldn't fundementally change the linear transformation a matrix represents.
  We can consider representations $\rho, \phi$ equivalent (through an equivalence relation) if there exists an invertible matrix $T$
  such that $\rho(g) = T^{-1} \phi(g) T$ for all $g \in G$.\par
  This relationship between matrices and linear transformations can be further used to avoid a choice of basis; we now introduce
  $G$-modules.
  \begin{defn}[{\cite[Definition 1.3.1]{sagan}}]
    Let $V$ be a vector space and $G$ be a group. Then $V$ is a \emph{$G$-module} if there is a group homomorphism
    \[\rho: G \to \GL(V),\]
    where $\GL(V)$ is the set of general linear transformations of the vector space $V$.
    Equivalently, V is a $G$-module if there is a multiplication $g \cdot v$ such that
    \begin{enumerate}
    \item $g \cdot v \in V$,
    \item $g(cv + dw) = c(gv) + d(gw)$,
    \item $(gh)v = g(hv)$, and
    \item $ev = v$
    \end{enumerate}
    for all $g,h \in G$, $v,w \in V$, and scalars $c,d \in \CC$.
    That is, there is a left action $G \acts V$ on $V$ that distributes over addition.
  \end{defn}
  These two definitions are equivalent because acting by a group element $g$ is equivalent to applying the linear transformation $\rho(g)$.
  $G$-modules and matrix representations are closely related. We use any matrix representation $X$ with degree matching the dimension of $V$
  to construct a $G$-module by defining the linear action $g \cdot v := X(g) v$. Likewise, if we have a $G$-module we can define
  a matrix representation $X$ by choosing a basis $B$ for the linear transformations $\rho(g)$ and then construct $X(g)$.
  An important difference between representations and modules is that we have a choice of vector space when making a module.
  We can use this fact (and the fact that actions are part of our module definition) to naturally represent arbitary group actions.
  \begin{ex} We turn a group action $G \acts X$ into a $G$-module by first making the vectors of our space be
    formal linear sum of elements of the set: $[x] = \sum_{x_i \in X} \lambda_i x_i$.
    The group action then can then be linearly extended to act on $\CC[X]$, the vector space generated by $X$ over $\CC$:
    \begin{align*}
      G &\acts \CC [X] \\
      (g,[x]) &\mapsto \sum_{x_i \in X} \lambda_i (g \cdot x_i)
    \end{align*}
    Now we can define a $G$-module
    \[ \rho: G \to \GL(|X|, \CC [X])\]
    where $\rho(g)$ is the linear transformation induced by the action of $G$ on $X$; if we take the standard basis of $\CC [X]$ to be
    the set $X$, then $\rho(g)$ can be constructed as a matrix which permutes each $x_i$ according to the image of the action of $g$ on each $x_i$.
  \end{ex}
  We will be concerned with finding \emph{irreducible} representations/modules,
  which are fundemental to understanding the representation theory of a group because
  for representations over $\CC$ and similarly nice fields any representation can be represented as a product of irreducible representations.
  We will concern ourselves with irreducible modules primarily because they are easier to work with and are in close correspondence
  with irreducible representations. To do so we first need submodules.
  \begin{defn}[{\cite[Definition 1.4.1]{sagan}}]
    Let $V$ be a $G$-module. a \emph{submodule}
    of $V$ is a subspace $W$ that is closed under the action of $G$; that is,
    \[w \in W \implies gw \in W \text{ for all } g \in G.\]
  \end{defn}
  \begin{ex}
    Any $G$-module $V$ has the submodules $W=V$ as well as $W= \{0\}$, where $0$ is the zero vector.
    These two submodules are called trivial, and any other submodules are called nontrivial.
  \end{ex}
  \begin{defn}[{\cite[Definition 1.4.1]{sagan}}]
    A nonzero $G$-module $V$ is \emph{reducible} if it contains a non-trivial submodule $W$.
    Otherwise $V$ is said to be reducible.
  \end{defn}

  We will need the following definition in our analysis of $S_n$ to demonstrate that we have found a complete set of irreducible non-isomorphic $G$($S_n$)-modules.

  \begin{defn}[{\cite[Definition 1.6.1]{sagan}}]
    Let $V$ and $W$ be $G$-modules. Then a \emph{$G$-homomorphism} is a linear transformation $\Theta: V \to W$ such that
    \[\Theta(g \cdot v) = g \cdot \Theta(v)\]
    for all $g \in G$ and $v \in V$.\\
    A \emph{$G$-isomorphism} is a bijective $G$-homomorphism.
  \end{defn}

  Finally, we introduce two results without proof that we will need for the representation theory of $S_n$. The first one, called
  Matchske's theorem, is the theorem that tells us that any representation can be constructed out of irreducible representations.
  To understand it we first need the notion of a direct sum.
  \begin{defn}[{\cite[Definition 1.5.1]{sagan}}]
    Let $V$ be a vector space with subspaces $U$ and $W$.
    Then $V$ is the \emph{(internal) direct sum of $U$ and $W$}, written $V = U \oplus W$, if every $v \in V$ can be
    written uniquely as a sum
    \[v= u +w, \quad u \in U, w \in W.\]
  \end{defn}
  In the same way that the tensor product is like multiplication for vector spaces, the direct sum is analagous to the addition of vector spaces.
  \begin{thm}[{\cite[Theorem 1.5.3]{sagan}; Matchske's theorem}]
    Let $G$ be a finite group and let $V$ be a nonzero $G$-module. Then
    \[ V = W_1 \oplus W_2 \oplus \dots \oplus W_k\]
    where each $W_i$ is an irreducible $G$-submodule of $V$.
  \end{thm}

  This next proposition gives us results from character theory which characterize the relationship between irreducible representations
  and their corresponding group.

  \begin{prop}[{\cite[Proposition 1.10.1]{sagan}}]
    Let $G$ be a finite group and suppose that the set $V_i$ ranging over $i$ forms a complete list of
    distinct irreducible $G$-modules. Then
    \begin{enumerate}
    \item $\sum_i (\dim V_i)^2 = |G|$, and
    \item the number of $V_i$ equals the number of conjugacy classes of $G$.
    \end{enumerate}
  \end{prop}   
  \todo{ need to introduce character theory}

  \section{Representation Theory of the Symmetric Group}
  \subsection{Motivation}\label{motivation} \hfill\\
  We will consider the symmetric group $S_n$ and its representations; in doing so, we will find how these representations correspond to mathematical objects known as Young tableaux. \par
  We state without proof that the number of irreducible representations (up to isomorphism) of a group is equal to the number of conjugacy classes in
  the group \cite[Proposition 1.10.1]{sagan}; this comes from character theory outside of our scope\todo{not out of scope, explain this}. Fundemental abstact algebra tells us that 
  conjugacy classes of $S_n$ are in bijection with the cycle types of $S_n$,
  and because cycle types are unordered lists we can map each cycle type to its unique weakly decreasing ordering.
  This forms a bijection between cycle types of $S_n$ and integer partitions of $n$.
  Composing bijections, we can conclude that each integer partition of $n$ uniquely corresponds to an irreducible representation of $S_n$! \par
  Our goal will be to find these irreducible representations by creating their corresponding irreducible $S_n$-modules.
  \subsection{Tableaux and Tabloids} \hfill\\
  In order to use tableaux in $S_n$-modules, we'll need to define the action of $S_n$ on a tableaux of a partition of size $n$.
  \todo{define young diagrams, relation to partitions}
  \begin{defn}
    A \emph{Young Tableaux} is a numbering of a Young diagram of size $n$ with the numbers $\{1 \cdots n\}$ with no
    repeats allowed. \par
    There is no ordering condition, unlike with standard Young tableaux (hereafter refered to as SYT);
    we go out of our way to define a plain tableaux because the definition of a tableaux can be inconsistent across sources.
  \end{defn}
  The action of $\sigma \in S_N$ on a tableaux $T$ of size $n$ is to replace the box $i$ in $T$ with the box $\sigma(i)$ in $\sigma \cdot T$.
  \begin{defn}[{\cite[Pg. 84]{fulton}}]
    The \emph{row group} of $T$, denoted $R(T)$, is the set of permutations which permute the entries of each row among themselves.
    If $\lambda = ( \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_k < 0)$, then $R(T)$ is a product of symmetric groups
    $S_{\lambda_1} \times S_{\lambda_2} \times \cdots \times S_{\lambda_k}$.
    In fact, if row 1 of $T$ contains the numbers $\{ j_1 \cdots i_1\} \subseteq [n]$, row 2 contains $\{j_2 \cdots i_2 \}$, et cetra,
    then $R(T)$ is the product of the automorphisms each set --- $S_{\{ j_1 \cdots i_1\}} \times S_{\{ j_2 \cdots i_2\}} \times \cdots \times S_{\{ j_k \cdots i_k\}}$.\par
    We analagously define the column group of $T$, $C(T)$, to be the set of permutations which permute the entries of the columns among themselves.
    This is equivalent to the row group of the transpose of the tableaux.
  \end{defn}
  These subgroups of $S_n$ are compatible with the action of $S_n$ on $T$ in the following way:
  \[ R(\sigma \cdot T) = \sigma \cdot R(T) \cdot \sigma^{-1} \text{ and } C(\sigma \cdot T) = \sigma \cdot C(T) \cdot \sigma^{-1}.\]
  \begin{ex}
    For the tableaux below,
    $(143) \in R(T)$ but $(12) \notin R(T)$.
    Analagously $(143) \notin C(T)$ and $(12) \in C(T)$.
    \[
      \begin{ytableau}
        1 & 4 & 3 \\
        2 & 5 \\
      \end{ytableau}
    \]
    If we perform the action $(12)(34) \cdot T$ we now see that $\sigma (143) \sigma^{-1} = (12)(34) (143) (12)(34) = (243) \in R(\sigma \cdot T)$.
    Intuitively conjugation is swapping elements of the underlying set undergoing an automorphism.
    \[
      \begin{ytableau}
        2 & 3 & 4 \\
        1 & 5 \\
      \end{ytableau}
    \]
  \end{ex}
  We now define tabloids, which will be used in our construction of $S_n$-modules.
  \begin{defn}[{\cite[Pg. 85]{fulton}}]
    A \emph{tabloid} is an equivalence class of tableaux where two tableaux of the same shape are equivalent if their rows contain the same
    values. The tabloid containing $T$ is denoted $\{T \}$. Two tableaux are in the same class ($\{ T \} = \{ T'\}$) when $T'  = \sigma \cdot T$ for
    some $\sigma \in R(T)$.
  \end{defn}

  \begin{ex}
    \ytableausetup{tabloids,centertableaux}

    Tabloids are notated as tableaux without vertical lines to convey that changing the ordering of values within a row doesn't change the tabloid.
    \[
      \begin{ytableau}
        1 & 4 & 3 \\
        2 & 5 \\
      \end{ytableau}    =
      \begin{ytableau}
        4 & 1 & 3 \\
        2 & 5 \\
      \end{ytableau}
      \neq
      \begin{ytableau}
        4 & 5 & 3 \\
        2 & 1 \\
      \end{ytableau}
    \]
  \end{ex}

  \subsection{The modules $M^\lambda$ and $S^\lambda$} \hfill\\
  Before defining $M^\lambda$,  we must first linearly extend $S_n$ to the group ring $\CC[S_n]$, the
  formal linear combinations of permutations with coefficents in $\CC$ --- $\sum x_\sigma \sigma$.
  Multiplication in the group ring is determined by composition.
  Note that we can restrict a $\CC[S_n]$-module to an $S_n$-module by restricting $\CC[S_n]$ to the set
  \[\{1_\CC \sigma | \sigma \in S_n\}\]
  which is isomorphic to $S_n$, so we still obtain our desired $S_n$-module and can construct any desired matrix representations of $S_n$.

  \begin{defn}[{\cite[Pg. 86]{fulton}}]
    We let $M^\lambda$ denote the complex vector space with basis the set of tabloids $\{T\}$ for a given partition $\lambda$ size $n$.\par
    Since $S_n$ acts on the set of tabloids, it acts on $M^\lambda$, and we can linearly extend this to an action of $\CC[S_n]$ on $M^\lambda$;
    namely
    \[ (\sum x_\sigma \sigma) \cdot (\sum x_T \{T\}) = \sum \sum x_\sigma x_T \{ \sigma \cdot T \}.\]
    Therefore $M^\lambda$ is a $\CC[S_n]$-module. 
  \end{defn}
  We now need some special elements to construct our desired submodule of $M^\lambda$:
  \begin{defn}[{\cite[Pg. 86]{fulton}}]
    Given a tableaux $T$, we define the element $ b_T \in \CC[S_n]$:
    \[b_T = \sum_{q \in C(T)} \sgn(q)q.\]
    This is a \emph{Young symmetrizer}; there are two others we have left undefined as they are used in the symmetric (column-wise)
    definition of tabloids outside of our scope.
  \end{defn}

  \begin{defn}[{\cite[Pg. 86]{fulton}}]
    For each tableaux (not tabloid!) $T$ of shape $\lambda$ there is an element $v_T \in M_\lambda$
    defined by the formula
    \[v_T = b_T \cdot \{T\} = \sum \sgn(q) \{q \cdot T\}.\]
    Note that changing $T$ might not change the tabloid $\{T\}$ but could change $b_T$ resulting in a different element in $M^\lambda$.
  \end{defn}

  We can now define the subspace $S^\lambda$:
  \begin{defn}[{\cite[Pg. 87]{fulton}}]
    The \emph{Specht module}, denoted \emph{$S^\lambda$}, is the subspace of $M^\lambda$ spanned by the elements $v_T$, as $T$ varies over all tableaux of
    $\lambda$.
  \end{defn}
  To prove that $S^\lambda$ is actually a module, we need to show that it is closed under the action of $\CC[S_n]$; namely,
  we will show that $\sigma \cdot v_T = v_{\sigma \cdot T}$ for all tableaux $T$ and $\sigma \in S_n$.
  \begin{proof}
    Recall that $C(\sigma \cdot T) = \sigma \cdot C(T) \cdot \sigma^{-1}$; we first observe that
    \begin{align*}
      \sigma \cdot v_{T} &= \sigma \cdot b_T \cdot \{ T\} \\
                         &= \sigma \cdot \sum_{ q \in C(T)} \sgn(q) \{q \cdot T\} \\
                         &= \sum_{ q \in C(T)} \sgn(q) \{\sigma \cdot q \cdot T\}. \\
    \end{align*}
    On the other hand,
    \begin{align*}
      v_{\sigma \cdot T} &= b_{\sigma \cdot T} \{ \sigma \cdot T\} \\
                         &= \sum_{q \in C(\sigma \cdot T)} \sgn(q) \{q \cdot \sigma \cdot T\}\\
                         &= \sum_{q \in \sigma \cdot C(T) \cdot \sigma^{-1}} \sgn(q) \{q \cdot \sigma \cdot T\}\\
                         &= \sum_{q \in C(T)} \sgn(\sigma q \sigma^{-1}) \{\sigma q \sigma^{-1} \cdot \sigma \cdot T\}\\
                         &= \sum_{q \in C(T)} \sgn(\sigma q \sigma^{-1}) \{\sigma \cdot q \cdot T\}\\      
    \end{align*}
    We cite the abstact alegebra fact that conjugation doesn't change the sign of a permutation, so these two expressions are equal.
  \end{proof}

  Because $\sigma \cdot v_T = v_{\sigma \cdot T} \in S^\lambda$, $S^\lambda$ is closed under $S_n$. Furthermore $S^\lambda$ is closed under $\CC [S_n]$, as we can
  now calculate
  \[(\sum x_\sigma \sigma) \cdot v_T = \sum x_\sigma v_{\sigma \cdot T} \in S^\lambda.\]
  Because $S^\lambda$ is a subspace of $M^\lambda$, we have thus proven that
  \begin{thm}
    $S^\lambda$ is a $\CC[S_n]$ submodule of $M^\lambda$.
  \end{thm}
  In fact, we know that $S^\lambda = \CC[S_n] \cdot v_T$ for \emph{any given} tableaux $T$,
  because we can create any desired element in $M^\lambda$ like so:
  \[\sum_{\{T'\} \in A \subseteq \text{ set of tabloids}} x_{T'} v_{T'} = (\sum x_{T'} \sigma_{T'}) \cdot v_T,\]
  where $\sigma_{T'} \cdot v_T = v_{\sigma_{T'} \cdot T} = v_{T'}$.
  \subsection{Irreducibility and completeness of the $S^\lambda$ modules} \hfill\\
  We now need to show that the modules $S^\lambda$ have our desired properties: every $S^\lambda$ is irreducible, no two $S^\lambda$ are isomorphic,
  and any irreducible representation is isomorphic to some $S^\lambda$. Once we do this we will have shown that the set of modules $S^\lambda$
  over partitions of a given size $n$
  are in bijection with the irreducible representations of $S_n$ (up to isomorphism)! \par
  The idea of our proof will be to show that each $S^\lambda$ is indecomposable (contains no nontrivial submodules)
  and distinct by putting a linear order on the tableaux of any shape
  and of size $n$. Matchske's theorem tells us that indecomposibility is equivalent to irreducibility, and we will use some properties of the ordering
  to show that each $S^\lambda$ is disjoint to $S^{\lambda'}$ when $\lambda < \lambda'$ and $\lambda \neq \lambda'$.
  \par
  We begin by defining the lexicographical and dominance orderings:
  \begin{defn}[ {\cite[pg. 36]{fulton}} ]
    The \emph{lexicographic} ordering on partitions of size $n$, denoted $ \lambda \leq \lambda'$, means that
    for the first $i$ for which $\lambda_i \neq \lambda'_i$ (if any), has $\lambda_i < \lambda'_i$ (in the standard ordering on integers).
    It is a linear order.
  \end{defn}
  \begin{defn}
    The \emph{dominance} ordering on partitions of size $n$, denoted $ \lambda \normeq \lambda'$ or ``$\lambda'$ dominates $\lambda$'',
    means that $\sum_{1 \leq j \leq i} \lambda_j \leq \sum_{1 \leq j \leq i} \lambda'_j$ for all $ 1 \leq i \leq \infty$. \par
    The intuition here is that parititions with a few long rows dominate partitions with many short rows; note, however, that this is
    not a linear order. For example, we see for the partitions $\lambda = (4,1,1,1) \vdash 7$ and $\lambda' = (3,3,1) \vdash 7$ that
    \begin{align*}
      &4 \not \leq 3, \text{ so $\lambda \not \normeq \lambda'$}\\
      &3 \leq  4, \ 3 + 3 \leq 4 + 1  \text{ so $\lambda' \not \normeq \lambda$}\\
    \end{align*}
    so $\lambda$ and $\lambda'$ are incomparable.
  \end{defn}
  We can now state the following lemma:
  \begin{lemma}[ {\cite[Lemma 7.1]{fulton}} ] \label{lem1}
    Let $T$ and $T'$ be tableaux of shape $\lambda$ and $\lambda'$ respectively, each of size $n$. Note that they can have different shape!
    Assume that $\lambda$ does not strictly dominate $\lambda'$.
    Then exactly one of the following occurs:
    \begin{enumerate}
    \item There are two distinct integers that occur in the same row of $T'$ and the same column of $T$.
    \item $\lambda = \lambda'$, and there is some $p'$ in $R(T)$ and some $q$ in $C(T)$ such that $p' \cdot T' = q \cdot T$.
    \end{enumerate}
  \end{lemma}
  \begin{proof}
    Suppose 1 is false. The entries of the first row of $T'$ must occur in different columns of $T$,
    so there is a $q_1 \in C(T)$ so that these entires occur in the first row of $q \cdot T$.\par
    The entries of the second row of $T'$ occur in different columns in $T$, and so they also occur in different columns of $q_1 \cdot T$,
    so there is a $q_2 \in C(q_1 \cdot T) = C(T)$ that:
    \begin{enumerate}
    \item Doesn't move entries in $q_1 \cdot T$ that are also in the first row of $T'$.
    \item Moves entries in the second row of $T'$ that are in $q_1 \cdot T$
      into the second row of $q_1 \cdot T$.
    \end{enumerate}
    We can repeat this process to obtain $q_1, \dots, q_k$ such that the entries in the first $k$ rows of $T'$ occur in the first $k$ rows of
    $q_k \cdot q_{k-1} \dots \cdot q_1 \cdot T$. The actions of $q_1, \dots, q_k$ don't change the shape of $T$, so we can deduce that
    \[\sum_{1 \leq j \leq k} \lambda'_j \leq \sum_{1 \leq j \leq k} \lambda_j\]
    because each row of $\lambda$ must have at least enough boxes to contain all the entries of the corresponding row of $\lambda'$.
    This holds for all $k$, so by definition $\lambda$ dominates $\lambda'$. \par
    We assumed that $\lambda$ doesn't strictly dominate $\lambda'$, so the only possibility is that $\lambda = \lambda'$.
    We can thus take $k$ to be the number of rows in $\lambda$ and let $q = q_k \cdots q_1$ so that
    $q \cdot T$ and $T'$ have the same entries in each row.
    We can now conclude that there is some $p' \in R(T)$ such that $p' \cdot T' = q \cdot T$.
  \end{proof}
  We now define our needed linear ordering; note that this ordering is on all tableaux of size $n$, not just partitions!
  \begin{defn}[ { \cite[pg. 84-85]{fulton} } ]
    We denote the linear ordering $T < T'$ on tableaux to mean either:
    \begin{enumerate}
    \item The shape of $T'$ is larger than the shape of $T$ in the lexicographical order.
    \item $T$ and $T'$ have the same shape, and the largest entry that is in a different box in the two numberings occurs
      earlier in the column word of $T'$ than in the column word of $T$. \\
      The column word is obtained by listing the entries of each column from bottom to top, reading columns from left to right.
    \end{enumerate}
  \end{defn}
  \begin{ex}
    This ordering puts the standard tableux of shape $(3,2)$ in the following order:
    \ytableausetup{notabloids,centertableaux}
    \[
      \begin{ytableau}
        1 & 2 & 3 \\
        4 & 5 \\
      \end{ytableau}
      >
      \begin{ytableau}
        1 & 2 & 4 \\
        3 & 5 \\
      \end{ytableau}
      >
      \begin{ytableau}
        1 & 3 & 4 \\
        2 & 5 \\
      \end{ytableau}
      >
      \begin{ytableau}
        1 & 2 & 5 \\
        3 & 4 \\
      \end{ytableau}
      >
      \begin{ytableau}
        1 & 3 & 5 \\
        2 & 4 \\
      \end{ytableau}.
    \]
    We demonstrate the first inequality by observing that $4$ is the largest entry in a different box between the two SYT,
    and that the first SYT has column word $41523$ while the second has column word $31524$.
    We see that $4$ occurs earlier in the column word of the first SYT, so the first SYT is ``larger'' in this ordering.

  \end{ex}
  An important property of this ordering for SYT $T$ and any $p \in R(T), q\in C(T)$ is that
  \[ p \cdot T < T \text{ and } q \cdot T > T.\]
  The symbol ``$<$'' does not denote a strict poset relation, so it could be that $p \cdot T = T$. \par
  This is true because the largest element moved by a row permutation must be moved left, pushing it closer to the front in the column word,
  while the largest element moved by a column permutation must be moved up, pushing it further back in the column word.
  \begin{ex}
    In the SYT below, any nontrivial row permutation will swap at least two elements.
    Because every element to the right of a given entry in a row is larger in a SYT, this swap will move the larger element to the left.
    Analogously, elements lower in a column are larger, so a column permutation will move the larger element up.
    \[
      (15) \cdot 
      \begin{ytableau}
        1 & 2 & 3 & 4 & 5 
      \end{ytableau}
      =
      \begin{ytableau}
        5 & 2 & 3 & 4 & 1 
      \end{ytableau}
    \]
  \end{ex}

  We need the following results for the next 2 proofs (both being excercises from \cite{fulton}):
  \begin{proof}
    We prove that, for all $q' \in C(T)$,
    \[q' \cdot b_T = \sgn(q') b_T.\]
    First we compute
    \[q' \cdot b_T = q' \cdot \sum_{q \in C(T)} \sgn(q)\{q \cdot T\} = \sum_{q \in C(T)} \sgn(q)\{q' \cdot q \cdot T\}.\]
    We know from abstract algebra that $q' C(T) = C(T)$ because $q' \in C(T) \leq S_n$; the function of applying $q'$ to every element of $C(T)$ is a
    bijection.
    Therefore the composition of $q'$ with every element of $C(T)$ will only reorder the addends in the sum.
    If $q'$ is odd, it will map each permutation to a permutation with opposite sign; if it is even, it will maintain the parity of each permutation.
    This is equivalent to multiplying  each permutation by the sign of $q'$, so
    \[\sum_{q \in C(T)} \sgn(q)\{q' \cdot q \cdot T\} = \sum_{q \in C(T)} \sgn(q)\sgn(q') \{q \cdot T\}
      = \sgn(q') \sum_{q \in C(T)} \sgn(q) \{q \cdot T\} = \sgn(q') b_T.\]
  \end{proof}
  \begin{proof}
    We prove that $b_T \cdot b_T = |C(T)| \cdot b_T$, where $\cdot$ here is multiplication in the group ring.
    We linearly extend the result we just proved:
    \[ b_T \cdot b_T = (\sum_{q \in C(T)} \sgn(q)q) \cdot b_T = |C(T)|\sgn(q)\sgn(q) \cdot b_T = |C(T)| \cdot b_T.\]  
  \end{proof}

  We now state and prove another lemma which applies our previous lemma to the $M^\lambda$ module:
  \begin{lemma}[{\cite[Lemma 7.2]{fulton}}]\label{lem2}
    Let $T$ and $T'$ be numberings of shapes $\lambda$ and $\lambda'$ respectively,
    and assume that $\lambda$ does not strictly dominate $\lambda'$. \\
    \begin{enumerate}

    \item If there is a pair of integers in the same row of $T'$ and the same column of $T$, then $b_T \cdot \{T'\} = 0$.
    \item If there is no such pair, then $b_T \cdot \{T'\} = \pm v_T$.
    \end{enumerate}
  \end{lemma}
  \begin{proof}    
    If there is such a pair of integers, let $t \in S_n$ be the transposition that swaps them.
    Then $b_T \cdot t = -b_T$, since $t$ is in the column group of $T$, and transpositions have odd sign.
    On the other hand $t \cdot \{T'\} = \{T'\}$ by the definition of a tabloid, because $t$ is in the row group of $T'$.
    Therefore
    \[b_T \cdot \{T'\} = b_T \cdot (t \cdot \{T'\}) = (b_T \cdot t) \cdot \{T'\} = - b_T \cdot \{T'\}\]
    so $b_T \cdot \{T'\} = 0$.\par
    If there is no such pair of integers, then let $p'$ and $q$ be as in the second case of our first lemma.
    Then
    \begin{align*}
      b_T \cdot \{ T'\} &=b_T \{p' \cdot T'\} = b_T \cdot \{ q \cdot T\} \\
                        &= b_T \cdot q \cdot \{T\} = \sgn(q) b_T \cdot \{T\} = \sgn(q) v_T.
    \end{align*}
  \end{proof}
  Finally we have the tools to prove our main theorem!
  \begin{thm}[{\cite[ Pg. 87-88]{fulton}}]
    For each partition $\lambda$ of $n$, $S^\lambda$ is an irreducible representation of $S_n$. Every irreducible representation of
    $S_n$ is isomorphic to exactly one $S_n$.
  \end{thm}
  \begin{proof}
    First, we note that no $v_T$ is $0$ by definition, so the modules $S^\lambda$ are all nonzero (nontrivial subspaces of $M^\lambda$).
    We wish to prove the following statements, for a given tableaux $T$ of $\lambda$:
    \begin{align}
      &b_T \cdot M^\lambda = b_T \cdot S^\lambda = \CC \cdot v_T \neq \{0\}. \\
      &b_T \cdot M^{\lambda'} = b_T \cdot S^{\lambda'} = \{0\} \text{ if $\lambda < \lambda'$ and $\lambda \neq \lambda'$,}
    \end{align}
    where $\{0\}$ is the trivial zero subspace.
    We begin with the first equation. \par
    The first equality follows as such, using our result that $b_T \cdot b_T = |C(T)| \cdot b_T$:
    \begin{align*}
      b_T \cdot b_T &= |C(T)| \cdot b_T \iff \\
      \dfrac{1}{|C(T)|} \cdot b_T \cdot b_T &= b_T;\\
      b_T \cdot M^\lambda &= \dfrac{1}{|C(T)|} \cdot b_T \cdot b_T \cdot M^\lambda \\
                    &=\dfrac{1}{|C(T)|} \cdot b_T S^\lambda \\
                    &= b_T \cdot \dfrac{1}{|C(T)|}  \cdot S^\lambda \\
                    &= b_T \cdot S^\lambda
    \end{align*}
    where we can commute $\dfrac{1}{|C(T)|}$ because it is only a scalar; furthermore $\dfrac{1}{|C(T)|}  \cdot S^\lambda = S^\lambda$
    because vector spaces are invariant under scaling.
    The second equality follows because we know that for any $T,T'$ tableaux of $\lambda$, we know by \cref{lem1} there are not
    two distinct integers that appear in the same row of $T'$ and in the same column of $T$, so by \cref{lem2}
    $b_T \cdot \{T'\} = \pm v_T$.
    This means that $b_t \cdot M^\lambda = \CC v_T$,
    because every element in $M^\lambda$ is mapped to a scaling of $v_T$, and any we can obtain any scaling of $v_T$ via
    $b_T \cdot x \{T\} =x v_T \in \CC \cdot v_T$. \par
    For the second equation, the first equality follows from our argument regarding the second equation.
    The second equality follows because by assumption we are in case 1 of \cref{lem1} and therefore the first case of \cref{lem2},
    so $b_T \cdot \{T'\} = 0$ for all $\{T'\} M^\lambda$, and so $b_T \cdot M^\lambda = 0$.\par
    Now by Matchske's theorem we know that $S^\lambda = W_1 \oplus W_2 \oplus \dots \oplus W_k$ for irreducible submodules $W_j$.
    We see that
    \[\CC \cdot v_T = b_T \cdot S^\lambda = b_T \cdot W_1 \oplus b_T \cdot W_2 \dots \oplus b_T \cdot W_k,\]
    so one of the modules $W_j$ must contain $v_T$.
    If some $W_i$ contains $v_T$, then by the definition of a submodule $W_i = \CC [S_n] \cdot v_T = S^\lambda$, so
    the other submodules must be the zero submodule and so $S^\lambda$ is irreducible.
    \par
    Furthermore we prove that $S^\lambda  \ncong S^{\lambda'}$ for any $\lambda < \lambda'$ and $\lambda \neq \lambda'$.
    We assume there exists a module isomorphism $\Theta$ between $S^\lambda$ and $S^{\lambda'}$.
    By the definition of a module isomorphism, $\Theta(b_T \cdot S^{\lambda'}) = b_T \cdot \Theta( S^{\lambda'})$,
    but we just showed that
    \[\Theta(b_T \cdot S^{\lambda'}) = \Theta(0) = 0 \neq b_T \cdot S^\lambda = b_T \cdot \Theta( S^{\lambda'})\]
    which is a contradiction. Therefore $S^\lambda \ncong S^{\lambda'}$, and because $<$ is a linear ordering, we can conclude that
    no two distinct $S^\lambda$ are isomorphic. \par
    Finally we cite without proof \cite[Proposition 1.10.1]{sagan}, specifically the result that the number of irreducible modules/representations equals
    the number of conjugacy classes of the group. We have previously discussed how the conjugacy classes of $S_n$ are in bijection with cycle types of $S_n$,
    which in turn are in bijection with partitions of $n$.
    Because there is one Specht module $S^\lambda$ for each partition $\lambda$ of size $n$,
    we can conclude that there are exactly as many $S^\lambda$ as there are irreducible representations of $S_n$.
    That is, the set of modules $S^\lambda$ is all of the irreducible modules/representations of $S_n$ up to isomorphism.
  \end{proof}

  \subsection{A basis for $S^\lambda$} \hfill\\
  With the main result aside, we now prove a proposition that utilizes combinatorial results from class!
  \begin{prop}[{\cite[Pg. 88]{fulton}}]\label{prop1}
    The elements $v_T$, as $T$ varies over the \emph{standard} tableaux of $\lambda$, form a basis for $S^\lambda$.
  \end{prop}
  \begin{proof}
    The element $v_T$ is a linear combination of $\{T\}$, with coefficent $1$ (as the trivial permutation has even sign),
    and elements $\{q \cdot T\}$, for $q \in C(T)$, with coefficents $\pm 1$.
    Recall that when $T$ is a SYT, $q \cdot T < T$, and furthermore this relation is strict when $q$ is nontrivial.
    To find solutions to the equation $\sum_{T \in \text{ SYT of $\lambda$}} x_Tv_T = 0$,
    we can look at the largest $v_T$ with nonzero coefficent $x_T$.
    We know that the $\{T\}$ component cannot be canceled out by some other $v_{T'}$, because it must be that $\{T\} \neq \{T'\}$ in
    order for the relation $v_T > v_{T'}$ to be strict,
    and furthermore $\{T\} \neq \{q \cdot T'\}$ because we know $\{ T\} > \{T'\} > \{q \cdot T'\}$ and at least the first relation is strict.
    Thus $\{T\}$ cannot be canceled out by some other term, so it must be that $x_T = 0$; but then all $x_T = 0$ because
    if any $x_T$ is nonzero there will be some largest nonzero $v_T$.
    We can thus conclude that the elements $v_T \in S^\lambda$ as $T$ varies over the SYT are linearly independent. \par
    To show that these elements span $S^\lambda$, we again cite \cite[Proposition 1.10.1]{sagan}, specifically the result
    that
    \[ \sum_i (\dim V_i)^2 =|G| \]
    where the $V_i$ are a complete set of irreducible $G$-modules.
    In our case this means that
    \[\sum_\lambda (\dim S^\lambda)^2 = n!\]
    We proved in class that \todo{ I have to do this proof myself now}
    \[ \sum_\lambda(f^\lambda)^2 = n!\]
    where $f^\lambda$ is the number of SYT of the partition $\lambda$ of size $n$.
    We can thus conclude
    \[ n! = \sum_\lambda (\dim S^\lambda)^2 = \sum_\lambda(f^\lambda)^2 = n!.\]
    It follows that $\dim(S^\lambda) = f^\lambda$ for all $\lambda$, and because $f^\lambda$ counts the number of SYT of shape $\lambda$
    this means that the elements $v_T$ as $T$ varies over SYT must span $S^\lambda$.
  \end{proof}
  % In this proof we used our in-class proof that $\sum_\lambda(f^\lambda)^2 = n!$, but it's actually possible to prove \cref{prop1}
  % without this fact using the so-called straightening algorithm \cite[Chapter 7.4]{fulton}.
  % Taking that route leads to a representation-theoretic proof that $\sum_\lambda(f^\lambda)^2 = n!$,
  % because if we know $n! = \sum_\lambda (\dim S^\lambda)^2$ and that the elements $v_T$ over SYT form a basis for $S^\lambda$,
  % we can conclude that $\dim(S^\lambda) = f^\lambda$ and therefore $\sum_\lambda(f^\lambda)^2 = n!$. \par

  % That concludes the results of this paper. The most powerful tool that representation theory provides towards results in other disciplines
  % actually come from a subfield called character theory, which is concerned with the trace (sum of diagonal values) of matrix representations.
  % Character theory is needed to prove results in abstract algebra that we have otherwise found no methods of proving --- specifically, it
  % is needed for the classification of finite simple groups! As such the character theory of representations of $S_n$ is of great interest
  % and application both inside and outside representation theory. Two other jumping off points would be the straightening algorithm
  % discussed in \cite[Chapter 7.4]{fulton}, which is an application of something called Garnier elements, and the ring of
  % representations of $S_n$ and symmetric functions \cite[7.3]{fulton}, which allows for the application of symmetric functions
  % to the representation theory of $S_n$ and leads to results like Youngs rule, which tells us that irreducible representations of
  % $S_{n+1}$ or $S_{n-1}$ can be induced or restricted from representations of $S_n$ by taking direct sums of $S^\mu$ for greater or lesser covering partitions
  % of $\lambda$ of $n$ in Young's diagram!



  \chapter{Results}

















   









































































        
    \chapter{The First}
    	This is the first page of the first chapter. You may delete the contents of this chapter so you can add your own text; it's just here to show you some examples. 
	
\section{References, Labels, Custom Commands and Footnotes}
It is easy to refer to anything within your document using the \texttt{label} and \texttt{ref} tags.  Labels must be unique and shouldn't use any odd characters; generally sticking to letters and numbers (no spaces) should be fine. Put the label on whatever you want to refer to, and put the reference where you want the reference. \LaTeX\ will keep track of the chapter, section, and figure or table numbers for you. 

\subsection{References and Labels}
Sometimes you'd like to refer to a table or figure, e.g. you can see in Figure \ref{subd2} that you can rotate figures . Start by labeling your figure or table with the label command (\verb=\label{labelvariable}=) below the caption (see the chapter on graphics and tables for examples). Then when you would like to refer to the table or figure, use the ref command (\verb=\ref{labelvariable}=). Make sure your label variables are unique; you can't have two elements named ``default." Also, since the reference command only puts the figure or table number, you will have to put  ``Table" or ``Figure" as appropriate, as seen in the following examples:

 As I showed in Table \ref{inheritance} many factors can be assumed to follow from inheritance. Also see the Figure \ref{subd} for an illustration.
 
\subsection{Custom Commands}\label{commands}
Are you sick of writing the same complex equation or phrase over and over? 

The custom commands should be placed in the preamble, or at least prior to the first usage of the command. The structure of the \verb=\newcommand= consists of the name of the new command in curly braces, the number of arguments to be made in square brackets and then, inside a new set of curly braces, the command(s) that make up the new command. The whole thing is sandwiched inside a larger set of curly braces. 

% Note: you cannot use numbers in your commands!
\newcommand{\hydro}{H$_2$SO$_4$}

In other words, if you want to make a shorthand for H$_2$SO$_4$, which doesn't include an argument, you would write: \verb=\newcommand{\hydro}{H$_2$SO$_4$}= and then when you needed  to use the command you would type \verb=\hydro=. (sans verb and the equals sign brackets, if you're looking at the .tex version). For example: \hydro

\subsection{Footnotes and Endnotes}
	You might want to footnote something.\footnote{footnote text} Be sure to leave no spaces between the word immediately preceding the footnote command and the command itself. The footnote will be in a smaller font and placed appropriately. Endnotes work in much the same way. More information can be found about both on the CUS site.
	
\section{Bibliographies}
	Of course you will need to cite things, and you will probably accumulate an armful of sources. This is why BibTeX was created. For more information about BibTeX and bibliographies, see our CUS site (\url{web.reed.edu/cis/help/latex/index.html}). There are three pages on this topic: {\it bibtex} (which talks about using BibTeX, at \url{/latex/bibtex.html}), {\it bibtexstyles} (about how to find and use the bibliography style that best suits your needs, at \url{/latex/bibtexstyles.html}) and {\it bibman} (which covers how to make and maintain a bibliography by hand, without BibTeX, at at \url{/latex/bibman.html}). The last page will not be useful unless you have only a few sources. There used to be APA stuff here, but we don't need it since I've fixed this with my apa-good natbib style file.
	
\subsection{Tips for Bibliographies}
\begin{enumerate}
\item Like with thesis formatting, the sooner you start compiling your bibliography for something as large as thesis, the better. Typing in source after source is mind-numbing enough; do you really want to do it for hours on end in late April? Think of it as procrastination.
\item The cite key (a citation's label) needs to be unique from the other entries.
\item When you have more than one author or editor, you need to separate each author's name by the word ``and'' e.g.\\ \verb+Author = {Noble, Sam and Youngberg, Jessica},+.
\item Bibliographies made using BibTeX (whether manually or using a manager) accept LaTeX markup, so you can italicize and add symbols as necessary.
\item To force capitalization in an article title or where all lowercase is generally used, bracket the capital letter in curly braces.
\item You can add a Reed Thesis citation option. The best way to do this is to use the phdthesis type of citation, and use the optional ``type'' field to enter ``Reed thesis'' or ``Undergraduate thesis''. Here's a test of Chicago, showing the second cite in a row being different. Also the second time not in a row should be different. Of course in other styles they'll all look the same.
\end{enumerate}
\section{Anything else?}
If you'd like to see examples of other things in this template, please contact CUS (email cus@reed.edu) with your suggestions. We love to see people using \LaTeX\ for their theses, and are happy to help.


\chapter{Mathematics and Science}	
\section{Math}
	\TeX\ is the best way to typeset mathematics. Donald Knuth designed \TeX\ when he got frustrated at how long it was taking the typesetters to finish his book, which contained a lot of mathematics. 
	
	If you are doing a thesis that will involve lots of math, you will want to read the following section which has been commented out. If you're not going to use math, skip over this next big red section. (It's red in the .tex file but does not show up in the .pdf.)

\chapter{Tables and Graphics}

\section{Tables}
	The following section contains examples of tables, most of which have been commented out for brevity. (They will show up in the .tex document in red, but not at all in the .pdf). For more help in constructing a table (or anything else in this document), please see the LaTeX pages on the CUS site. 

\begin{table}[htbp] % begins the table floating environment. This enables LaTeX to fit the table where it works best and lets you add a caption.
\caption[Correlation of Inheritance Factors between Parents and Child]{Correlation of Inheritance Factors between Parents and Child} 
% The words in square brackets of the caption command end up in the Table of Tables. The words in curly braces are the caption directly over the table.
\begin{center} 
% makes the table centered
\begin{tabular}{c c c c} 
% the tabular environment is used to make the table itself. The {c c c c} specify that the table will have four columns and they will all be center-aligned. You can make the cell contents left aligned by replacing the Cs with Ls or right aligned by using Rs instead. Add more letters for more columns, and pipes (the vertical line above the backslash) for vertical lines. Another useful type of column is the p{width} column, which forces text to wrap within whatever width you specify e.g. p{1in}. Text will wrap badly in narrow columns though, so beware.
\toprule % a horizontal line, slightly thicker than \hline, depends on the booktabs package
  Factors &  Correlation between Parents \& Child & Inherited \\ % the first row of the table. Separate columns with ampersands and end the line with two backslashes. An environment begun in one cell will not carry over to adjacent rows.
  \midrule % another horizontal line
	Education 				& -0.49 & Yes 	 \\ % another row
	Socio-Economic Status 	& 0.28 	& Slight \\
	Income 					& 0.08 	& No	 \\
	Family Size 			& 0.19 	& Slight \\
	Occupational Prestige 	& 0.21 	& Slight \\
\bottomrule % yet another horizontal line
\end{tabular}
\end{center}
\label{inheritance} % labels are useful when you have more than one table or figure in your document. See our online documentation for more on this.
\end{table}

	\clearpage 
%% \clearpage ends the page, and also dumps out all floats. 
%% Floats are things like tables and figures.

If you want to make a table that is longer than a page, you will want to use the longtable environment. Uncomment the table below to see an example, or see our online documentation.

   
   \section{Figures}
   
	If your thesis has a lot of figures, \LaTeX\ might behave better for you than that other word processor.  One thing that may be annoying is the way it handles ``floats'' like tables and figures. \LaTeX\ will try to find the best place to put your object based on the text around it and until you're really, truly done writing you should just leave it where it lies.   There are some optional arguments to the figure and table environments to specify where you want it to appear; see the comments in the first figure.

	If you need a graphic or tabular material to be part of the text, you can just put it inline. If you need it to appear in the list of figures or tables, it should be placed in the floating environment. 
	
	To get a figure from StatView, JMP, SPSS or other statistics program into a figure, you can print to pdf or save the image as a jpg or png. Precisely how you will do this depends on the program: you may need to copy-paste figures into Photoshop or other graphic program, then save in the appropriate format.
	
	Below we have put a few examples of figures. For more help using graphics and the float environment, see our online documentation.
	
	And this is how you add a figure with a graphic:
	\begin{figure}[h]
	% the options are h = here, t = top, b = bottom, p = page of figures.
	% you can add an exclamation mark to make it try harder, and multiple
	% options if you have an order of preference, e.g.
	% \begin{figure}[h!tbp]
	   
	       \centering
	    % DO NOT ADD A FILENAME EXTENSION TO THE GRAPHIC FILE
	    \includegraphics{subdivision}
	     \caption{A Figure}
	 \label{subd}
	\end{figure}

\clearpage %% starts a new page and stops trying to place floats such as tables and figures

\section{More Figure Stuff}
You can also scale and rotate figures.
 	\begin{figure}[h!]
	   
	       \centering
	    % DO NOT ADD A FILENAME EXTENSION TO THE GRAPHIC FILE
	    \includegraphics[scale=0.5,angle=180]{subdivision}
	    % if your figure shows up not where you want it, it may just be too big to fit. You can use the scale argument to shrink it, e.g. scale=0.85 is 85 percent of the original size. 
	     \caption{A Smaller Figure, Flipped Upside Down}
	 \label{subd2}
	\end{figure}

\section{Even More Figure Stuff}
With some clever work you can crop a figure, which is handy if (for instance) your EPS or PDF is a little graphic on a whole sheet of paper. The viewport arguments are the lower-left and upper-right coordinates for the area you want to crop.

 	\begin{figure}[h!]
	    	       \centering
	    % DO NOT ADD A FILENAME EXTENSION TO THE GRAPHIC FILE
	   \includegraphics[clip=true, viewport=.0in .0in 1in 1in]{subdivision}
	    \caption{A Cropped Figure}
	 \label{subd3}
	\end{figure}
	
      \subsection{Common Modifications}
      The following figure features the more popular changes thesis students want to their figures. This information is also on the web at \url{web.reed.edu/cis/help/latex/graphics.html}.
    %\renewcommand{\thefigure}{0.\arabic{figure}} 	% Renumbers the figure to the type 0.x
    %\addtocounter{figure}{4} 						% starts the figure numbering at 4
    \begin{figure}[htbp]
    \begin{center}
   \includegraphics[scale=0.5]{subdivision}
    \caption[Subdivision of arc segments]{\footnotesize{Subdivision of arc segments. You can see that $ p_3 = p_6^\prime$.}} %the special ToC caption is in square brackets. The \footnotesize makes the figure caption smaller
    \label{barplot}
    \end{center}
    \end{figure} 

\chapter*{Conclusion}
         \addcontentsline{toc}{chapter}{Conclusion}
	\chaptermark{Conclusion}
	\markboth{Conclusion}{Conclusion}
	\setcounter{chapter}{4}
	\setcounter{section}{0}
	
Here's a conclusion, demonstrating the use of all that manual incrementing and table of contents adding that has to happen if you use the starred form of the chapter command. The deal is, the chapter command in \LaTeX\ does a lot of things: it increments the chapter counter, it resets the section counter to zero, it puts the name of the chapter into the table of contents and the running headers, and probably some other stuff. 

So, if you remove all that stuff because you don't like it to say ``Chapter 4: Conclusion'', then you have to manually add all the things \LaTeX\ would normally do for you. Maybe someday we'll write a new chapter macro that doesn't add ``Chapter X'' to the beginning of every chapter title.

\section{More info}
And here's some other random info: the first paragraph after a chapter title or section head \emph{shouldn't be} indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there. 


%If you feel it necessary to include an appendix, it goes here.
    % \appendix
    %   \chapter{The First Appendix}
    %   \chapter{The Second Appendix, for Fun}


%This is where endnotes are supposed to go, if you have them.
%I have no idea how endnotes work with LaTeX.

  \backmatter % backmatter makes the index and bibliography appear properly in the t.o.c...

% if you're using bibtex, the next line forces every entry in the bibtex file to be included
% in your bibliography, regardless of whether or not you've cited it in the thesis.
    \nocite{*}

% Rename my bibliography to be called "Works Cited" and not "References" or ``Bibliography''
% \renewcommand{\bibname}{Works Cited}

%    \bibliographystyle{bsts/mla-good} % there are a variety of styles available; 
%  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can refer to files in the bsts or APA 
% subfolder, e.g. 
 \bibliographystyle{annotate}  % or
 \bibliography{thesisdsldesktop}
 % Comment the above two lines and uncomment the next line to use biblatex-chicago.
 %\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.
\end{document}
